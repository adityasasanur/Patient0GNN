{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GNNs to detect patient 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import ndlib.models.epidemics as ep\n",
    "import ndlib.models.ModelConfig as mc\n",
    "import ndlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/_5r1484j457cv8ysknrcyhvr0000gn/T/ipykernel_19739/2978814061.py:30: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matr = np.array(nx.adjacency_matrix(graph).todense())\n"
     ]
    }
   ],
   "source": [
    "# Generate 50 graphs, each with 50 nodes\n",
    "NUM_NODES = 50\n",
    "NUM_EDGES = 100\n",
    "NUM_ITERATIONS = 10\n",
    "NUM_TRAINING = 500\n",
    "BETA = 0.15\n",
    "GAMMA = 0\n",
    "\n",
    "graphs = np.zeros((NUM_TRAINING, NUM_NODES, NUM_NODES))\n",
    "nodes_statuses = []\n",
    "initial_infected = []\n",
    "for i in range(NUM_TRAINING):\n",
    "    graph = nx.gnm_random_graph(NUM_NODES, NUM_EDGES)\n",
    "    p0= np.random.randint(NUM_NODES)\n",
    "\n",
    "    config = mc.Configuration()\n",
    "    config.add_model_initial_configuration(\"Infected\", [p0])\n",
    "    config.add_model_parameter(\"beta\", BETA)\n",
    "    config.add_model_parameter(\"gamma\", GAMMA)\n",
    "\n",
    "    model = ep.SIRModel(graph)\n",
    "    model.set_initial_status(config)\n",
    "\n",
    "    iterations = model.iteration_bunch(NUM_ITERATIONS)\n",
    "    statuses = [iteration['status'] for iteration in iterations]\n",
    "    union_of_statuses = {k:0 for k in range(NUM_NODES)}\n",
    "    for status in statuses:\n",
    "        union_of_statuses.update(status)\n",
    "\n",
    "    adj_matr = np.array(nx.adjacency_matrix(graph).todense())\n",
    "    graphs[i,:,:] = adj_matr  # Convert sparse matrix to dense matrix\n",
    "    nodes_statuses.append([v for _,v in sorted(union_of_statuses.items(), key=lambda item: item[0])])\n",
    "    initial_infected.append([1 if i==p0 else 0 for i in range(NUM_NODES)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GCN layer\n",
    "class GraphConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolutionLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "\n",
    "        # Initialize learnable parameters\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, adj_matrix):\n",
    "        # Perform graph convolution\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj_matrix, support)  # Propagate information through the graph\n",
    "        output = output + self.bias\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphConvolutionalNetwork, self).__init__()\n",
    "        self.gcn1 = GraphConvolutionLayer(input_dim, hidden_dim)\n",
    "        self.gcn2 = GraphConvolutionLayer(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj_matrix):\n",
    "        x = F.relu(self.gcn1(x, adj_matrix))\n",
    "        x = self.gcn2(x, adj_matrix)\n",
    "        return x\n",
    "\n",
    "# Define a simple training loop\n",
    "def train_model(model, features, adj_matrix, labels, num_epochs, learning_rate):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features, adj_matrix)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    # print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avergae loss of epoch 0: 0.22527151116728783\n",
      "Avergae loss of epoch 1: 0.15915870533138513\n",
      "Avergae loss of epoch 2: 0.17424815061688423\n",
      "Avergae loss of epoch 3: 0.1506976108700037\n",
      "Avergae loss of epoch 4: 0.15587790777534247\n",
      "Avergae loss of epoch 5: 0.18814101070538164\n",
      "Avergae loss of epoch 6: 0.18447222628071905\n",
      "Avergae loss of epoch 7: 0.1691798877827823\n",
      "Avergae loss of epoch 8: 0.20190815620869398\n",
      "Avergae loss of epoch 9: 0.17927497444674373\n",
      "Avergae loss of epoch 10: 0.17957153210043908\n",
      "Avergae loss of epoch 11: 0.16776465348526837\n",
      "Avergae loss of epoch 12: 0.1644793018065393\n",
      "Avergae loss of epoch 13: 0.16751183831691743\n",
      "Avergae loss of epoch 14: 0.17249364118650556\n",
      "Avergae loss of epoch 15: 0.1742847158908844\n",
      "Avergae loss of epoch 16: 0.1806300436295569\n",
      "Avergae loss of epoch 17: 0.18660266072303056\n",
      "Avergae loss of epoch 18: 0.1952404471002519\n",
      "Avergae loss of epoch 19: 0.20462215889245272\n",
      "Avergae loss of epoch 20: 0.21529127330332995\n",
      "Avergae loss of epoch 21: 0.22524761686474085\n",
      "Avergae loss of epoch 22: 0.23467256772145628\n",
      "Avergae loss of epoch 23: 0.24168443673104048\n",
      "Avergae loss of epoch 24: 0.25194297383725645\n",
      "Avergae loss of epoch 25: 0.2569295021034777\n",
      "Avergae loss of epoch 26: 0.2543077458888292\n",
      "Avergae loss of epoch 27: 0.2227680623680353\n",
      "Avergae loss of epoch 28: 0.22588035204634072\n",
      "Avergae loss of epoch 29: 0.22254213329032063\n",
      "Avergae loss of epoch 30: 0.22067077580466868\n",
      "Avergae loss of epoch 31: 0.21497150157392025\n",
      "Avergae loss of epoch 32: 0.20743792854249476\n",
      "Avergae loss of epoch 33: 0.1984009709917009\n",
      "Avergae loss of epoch 34: 0.16470023638382553\n",
      "Avergae loss of epoch 35: 0.17069069459289313\n",
      "Avergae loss of epoch 36: 0.1731944067925215\n",
      "Avergae loss of epoch 37: 0.17611252711713313\n",
      "Avergae loss of epoch 38: 0.17701947335153817\n",
      "Avergae loss of epoch 39: 0.184135836225003\n",
      "Avergae loss of epoch 40: 0.18822477393224835\n",
      "Avergae loss of epoch 41: 0.1931307900287211\n",
      "Avergae loss of epoch 42: 0.2009039753191173\n",
      "Avergae loss of epoch 43: 0.18976574003696442\n",
      "Avergae loss of epoch 44: 0.21208405064418911\n",
      "Avergae loss of epoch 45: 0.21025502419099212\n",
      "Avergae loss of epoch 46: 0.2122299602255225\n",
      "Avergae loss of epoch 47: 0.20889205114170908\n",
      "Avergae loss of epoch 48: 0.2076071267351508\n",
      "Avergae loss of epoch 49: 0.2220599142536521\n",
      "Avergae loss of epoch 50: 0.23763577131181957\n",
      "Avergae loss of epoch 51: 0.23126226876303554\n",
      "Avergae loss of epoch 52: 0.21540865467488765\n",
      "Avergae loss of epoch 53: 0.22200413578748704\n",
      "Avergae loss of epoch 54: 0.24268118460848928\n",
      "Avergae loss of epoch 55: 0.2447783423922956\n",
      "Avergae loss of epoch 56: 0.2423748196810484\n",
      "Avergae loss of epoch 57: 0.23005698181688786\n",
      "Avergae loss of epoch 58: 0.2340929301828146\n",
      "Avergae loss of epoch 59: 0.24309349212795495\n",
      "Avergae loss of epoch 60: 0.2546202689372003\n",
      "Avergae loss of epoch 61: 0.25582752499356864\n",
      "Avergae loss of epoch 62: 0.2312862297669053\n",
      "Avergae loss of epoch 63: 0.21756546237319707\n",
      "Avergae loss of epoch 64: 0.23802803899347783\n",
      "Avergae loss of epoch 65: 0.23039549575001\n",
      "Avergae loss of epoch 66: 0.25791616168245673\n",
      "Avergae loss of epoch 67: 0.2689196342639625\n",
      "Avergae loss of epoch 68: 0.2472704021371901\n",
      "Avergae loss of epoch 69: 0.2478196549639106\n",
      "Avergae loss of epoch 70: 0.2413077239729464\n",
      "Avergae loss of epoch 71: 0.24696290431916715\n",
      "Avergae loss of epoch 72: 0.2355020478181541\n",
      "Avergae loss of epoch 73: 0.23504235742613674\n",
      "Avergae loss of epoch 74: 0.24011127511039376\n",
      "Avergae loss of epoch 75: 0.23105301498249173\n",
      "Avergae loss of epoch 76: 0.24166158561781048\n",
      "Avergae loss of epoch 77: 0.2171329793445766\n",
      "Avergae loss of epoch 78: 0.22224693225696682\n",
      "Avergae loss of epoch 79: 0.23435344108194112\n",
      "Avergae loss of epoch 80: 0.23233348033204676\n",
      "Avergae loss of epoch 81: 0.23471592701599\n",
      "Avergae loss of epoch 82: 0.2560359830036759\n",
      "Avergae loss of epoch 83: 0.25987574365362526\n",
      "Avergae loss of epoch 84: 0.24385008199885488\n",
      "Avergae loss of epoch 85: 0.2376281964071095\n",
      "Avergae loss of epoch 86: 0.23205169888958335\n",
      "Avergae loss of epoch 87: 0.23656907531619073\n",
      "Avergae loss of epoch 88: 0.22563049453869463\n",
      "Avergae loss of epoch 89: 0.23629976003989578\n",
      "Avergae loss of epoch 90: 0.23764898663759232\n",
      "Avergae loss of epoch 91: 0.22995262710377573\n",
      "Avergae loss of epoch 92: 0.23082106330245733\n",
      "Avergae loss of epoch 93: 0.22317956252023577\n",
      "Avergae loss of epoch 94: 0.21793121686577796\n",
      "Avergae loss of epoch 95: 0.22333918273448944\n",
      "Avergae loss of epoch 96: 0.22038746758922934\n",
      "Avergae loss of epoch 97: 0.225819141022861\n",
      "Avergae loss of epoch 98: 0.21614480198174715\n",
      "Avergae loss of epoch 99: 0.2162891441360116\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "# Load your graph data, features, adjacency matrix, and labels here\n",
    "# Replace the placeholders below with your data\n",
    "num_nodes = NUM_NODES\n",
    "num_features = 1\n",
    "num_classes = NUM_NODES\n",
    "model = GraphConvolutionalNetwork(num_features, 16, num_classes)\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for i in range(NUM_TRAINING):\n",
    "        features = torch.tensor(nodes_statuses[i]).reshape((NUM_NODES,1)).float()\n",
    "        adjacency_matrix = torch.tensor(graphs[i]).float()\n",
    "        labels = torch.tensor(initial_infected[i])\n",
    "        epoch_loss += train_model(model, features, adjacency_matrix, labels, num_epochs=1, learning_rate=0.01)\n",
    "    print(f\"Avergae loss of epoch {epoch}: {epoch_loss/NUM_TRAINING}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
